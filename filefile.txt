Quoc V. Le: Đúng vậy, như là như thế nào mà TV được chế tạo và như thế nào mà, bạn biết đấy, thang máy được làm ra như thế nào, hay làm thế nào mà con người chế tạo máy bay và những thứ như vậy, đúng không? Nên là điện năng, tôi rất tò mò về những công nghệ cơ bản này đã thay đổi cuộc sống của bạn, và rồi, bạn biết đấy, như là có, bạn biết đấy, tôi muốn, tôi muốn chia sẻ một chút bối cảnh vì những người trẻ ngày nay, họ lớn lên trong một thời kỳ mà những thứ đó là xa xỉ, thực ra chúng thậm chí còn không tồn tại hoặc không thể tiếp cận được, đúng không, đúng không?
Christopher Nguyen: Đúng, đúng vậy.
Quoc V. Le: Vậy nên khi tôi lớn lên, bạn biết đấy, làng của tôi, không có TV, như là chỉ có một chiếc TV cho cả làng thôi, và nhà tôi thì không có điện, và rồi bạn biết đấy, theo thời gian chúng tôi có điện khi tôi lớn lên, nhưng bạn biết đấy, mỗi, mỗi công nghệ mới xuất hiện, tôi cảm thấy như nó thay đổi cuộc sống, đúng không? Nên tôi, tôi trở nên tò mò về cách mà những công nghệ này ra đời vì lý do đó, và rồi tôi, bạn biết đấy, tôi đọc về, bạn biết đấy, sự kiện hạ cánh xuống mặt trăng, cái đó, bạn biết đấy, dự án Apollo, cái đã, đưa, đưa con người đầu tiên lên mặt trăng, đúng không? Và rồi tôi rất, tôi rất ấn tượng với bức ảnh đó, và bởi vì khi tôi nhìn vào nó, tôi, tôi nhận ra rằng, chúng ta không phải, bạn biết đấy, như là bằng cách nào đó chúng ta trở thành loài động vật đầu tiên đến được mặt trăng, mặc dù chúng ta không phải là, như là loài mạnh nhất hay chúng ta không phải là, chúng ta không, bạn biết đấy, chúng ta thậm chí không thể bay, đúng không? Như là, nhưng bằng cách nào đó chúng ta đến được mặt trăng, nên điều đó cho thấy sức mạnh của trí thông minh. Tất cả những gì xảy ra này thực ra là nhờ trí thông minh, nên tôi nhận ra rằng nếu, nếu bạn muốn xây dựng một công nghệ không có bất kỳ giới hạn nào, bạn phải giải mã được mã của trí thông minh, bởi vì nếu bạn có trí thông minh, bạn có thể giải quyết bất kỳ, bất kỳ thứ gì khác, bạn thực sự có thể xây dựng cái tiếp theo, bạn biết đấy, như là thuốc để chữa ung thư, bạn biết đấy, như là một vắc-xin mới, bạn biết đấy, cho, cho bất kỳ, bạn biết đấy, cúm mới, đúng không? À, hoặc là vật liệu mới, cho tấm pin năng lượng mặt trăng, đúng không? Hoặc pin, một số những công nghệ cơ bản này, vào một thời điểm nào đó sẽ chạm đến giới hạn của trí thông minh con người, và nếu bạn muốn tiến bộ cho nền văn minh của chúng ta, bạn thực sự muốn giải mã mã của trí thông minh, điều đó sẽ, cho chúng ta, bạn biết đấy, một con đường dài hơn, cho, bạn biết đấy, như là loài của chúng ta.
Christopher Nguyen: Tôi thực sự thích cái đó, cái hình ảnh của con người hay loài người chạm đến mặt trăng, nó làm tôi nhớ đến những gì Steve Jobs nói, máy tính trong quan điểm của ông ấy là như một chiếc xe đạp cho trí óc, câu chuyện của chính ông ấy, nên AI là con tàu vũ trụ cho trí óc, đúng không? Nhưng hãy kể cho chúng tôi nghe, làm thế nào bạn đến được Úc? Chuyện đó là như thế nào?
Quoc V. Le: Ồ, đúng vậy, tôi, bạn biết đấy, tôi đã học, trung học, và bạn biết đấy, tôi, tôi làm tốt trong một số kỳ thi, nên tôi nhận được một học bổng, như là, chính phủ, chính phủ Úc đã trao học bổng cho, bạn biết đấy, như là học sinh trung học, và tôi may mắn đủ để nhận được học bổng đó, và tôi đến Úc. Đúng vậy, nên cơ bản đó là câu chuyện, nhưng khi tôi ở, ở Úc, là nơi mà tôi lần đầu tiên phát hiện ra rằng, bạn biết đấy, tôi, tôi, tôi, tôi đăng ký vào, cái mà là Đại học Quốc gia Úc, nhưng phần thú vị về cái đó là nó không phải là một trường đại học tốt cho các chương trình đại học, mà thực ra là một trường đại học tốt cho nghiên cứu, nên tôi, và rồi tôi, tôi đã tính toán sai một chút vì tôi đang tìm kiếm một chương trình đại học tốt hơn, tôi đang tìm kiếm một, bạn biết đấy, một thành phố lớn hơn để sống, bởi vì tôi đã, tôi chán với một cái, tôi muốn đi sống ở một thành phố lớn hơn, và rồi tôi thấy Canberra là thủ đô của Úc, tôi nghĩ đây là một thành phố lớn, bạn biết đấy, thủ đô thì phải rất lớn, đúng không? Và rồi tôi đến Canberra, nó thực ra là một thành phố rất nhỏ, nó thực ra như là kích thước của chỗ tôi từng ở, nên nhưng một, một điều hóa ra là một quyết định không may mắn, bạn biết đấy, một quyết định tệ, hóa ra là một quyết định thực sự tốt, quyết định tuyệt vời vì họ có một chương trình nghiên cứu xuất sắc ở, ở Úc, và đó là nơi mà tôi, bạn biết đấy, có trải nghiệm, để làm việc với một số nhà nghiên cứu hàng đầu, bạn biết đấy, Úc, tỷ lệ giữa một sinh viên đại học và, bạn biết đấy, một người như giáo sư thực ra là, rất thấp, đúng không? Nên bạn có rất nhiều giáo sư để, để hướng dẫn, dưới, nên đó là rất đặc biệt.
Christopher Nguyen: Bây giờ, như là với các công ty khởi nghiệp, bạn biết đấy, mọi người nghĩ rằng có một câu nói rằng mỗi thành công qua đêm đều mất 10 năm, tôi nghĩ trong sách thì nó còn hơn cả 10 năm vì nghiên cứu của bạn không phải là một con đường thẳng, đúng không? Bạn vẫn đang tìm kiếm, ý tôi là bạn quan tâm đến trí thông minh, bạn biết đấy, thậm chí bạn rất không hài lòng với các phương pháp kernel, học thống kê và những thứ như vậy, và bạn đã đi đâu tiếp theo và bạn đã làm thế nào, đúng không?
Quoc V. Le: Vậy nên, khu vực đầu tiên mà tôi làm việc trong học máy là gọi là phương pháp kernel, và hồi đó, phương pháp kernel, thực ra là, bạn biết đấy, nếu bạn đến một hội nghị học máy như ICML hay NeurIPS, phương pháp kernel là một trong những nhánh phổ biến nhất, nếu không phải là phổ biến nhất, nên bạn biết đấy, nếu bạn viết một bài báo về phương pháp kernel, bạn có nhiều cơ hội hơn để thành công, được chấp nhận và những thứ như vậy, nhưng, theo thời gian tôi trở nên như là, tôi đi đến một nhận thức rằng nếu bạn thực sự muốn, xây dựng trí thông minh, bạn thực sự muốn học end-to-end, và một phần của phương pháp kernel mà tôi không thích là việc họ phải tạo kernel bằng tay, bằng tay, đúng không? Bạn cần chuyên môn để tạo cái kernel đó, nên tôi không thích điều đó, tôi nói chúng ta cần học end-to-end, chúng ta cần làm học end-to-end, bây giờ thực ra cái nhận thức đó xảy ra khi tôi đến thăm Viện Max Planck, ở Đức, nơi mà có rất nhiều nhà nghiên cứu làm việc về, hiểu não bộ, đó là họ có một phòng thí nghiệm mà họ, kiểu như nhìn vào não bộ của khỉ và những thứ như vậy, và một điều mà họ nói với tôi là thực ra não bộ, bạn biết đấy, bất kỳ bộ não nào, trong loài rất thông minh như chúng ta và, bạn biết đấy, như linh trưởng, có rất nhiều tính dẻo, cái từ là tính dẻo, bạn, những bộ não này học được rất nhiều, họ muốn, bạn biết đấy, chúng ta, chúng ta sinh ra trẻ, đúng không? Chúng ta không biết nhiều, bạn sinh ra ở Trung Quốc, có lẽ bạn học tiếng Trung và rồi bạn nói tiếng Trung rất tốt, sinh ra ở Việt Nam, bạn nói tiếng Việt, đúng không? Nên điều đó có nghĩa là bạn có thể học những thứ này rất tốt, đúng không? Từ, từ chỉ dữ liệu, nên rồi tôi, tôi vì tôi, và tôi rất bị cuốn hút bởi những cuộc thảo luận này nên tôi nhận ra rằng được rồi, tôi phải chuyển lĩnh vực, tôi phải chuyển từ phương pháp kernel sang, bạn biết đấy, mạng nơ-ron, thực ra lúc đầu khi tôi bắt đầu với mạng nơ-ron, lúc đầu là phương pháp kernel là một dạng đặc biệt của mạng nơ-ron, nên tôi thích nó rất nhiều, nhưng rồi tôi nhận ra rằng phương pháp kernel có rất nhiều hạn chế, nên tôi, tôi, ngừng làm việc với nó, chuyển sang làm việc với, mạng nơ-ron, và thực ra rất nhiều cố vấn của tôi và rất nhiều bạn bè của tôi nói rằng đó là như một quyết định ngu ngốc, tôi gần như sắp lấy được bằng tiến sĩ ở Úc, đúng không? Nếu tôi học thêm như một hoặc hai năm nữa tôi sẽ có được cái bằng tiến sĩ đó, nhưng rồi tôi nói không, tôi, tôi, tôi không thể, tôi không thể đi theo con đường đó vì tôi, bạn biết đấy, nó đi ngược lại niềm tin của tôi.
Christopher Nguyen: Và trong nhiều cách, sự thất vọng của bạn với cái này, cái sự khéo léo của con người được dẫn dắt, cao quý như nó vốn có, thực sự giống với Richard, bạn biết đấy, người đoạt giải Turing, đúng không? Cơ bản là ông ấy đã làm việc rất nhiều năm và ông ấy nói, bạn biết đấy, bài học đắng cay, tôi thực sự nghĩ ông ấy đã giành giải Turing cho bài blog bài học đắng cay đó, nơi mà ông ấy nói, bạn biết đấy, cơ bản là, tính toán và, và, và vấn đề tìm kiếm cuối cùng sẽ chiến thắng hơn bất kỳ loại nào, bất kỳ loại gì bạn có, nên, nên đó thực ra khá là sâu sắc và đủ niềm tin để đi vào cái gì đó mà mọi người đều khuyên bạn không nên làm, và bạn đã kết thúc ở Stanford như thế nào, đúng không?
Quoc V. Le: Vậy nên, trước hết tôi, bạn biết đấy, tôi, đôi khi tôi đến những hội nghị này, đúng không? Và rồi, tôi gặp, một số giáo sư và hồi đó nếu bạn đến NeurIPS hay ICML và, bạn biết đấy, có thể như một nhóm nhỏ người làm việc về học thật sự, mạng nơ-ron, nó là, bạn biết đấy, nếu bạn viết mạng nơ-ron trong đơn xin của bạn, bạn chắc chắn vào chương trình tiến sĩ, bạn gần như chắc chắn sẽ bị từ chối, nên mọi người không muốn nói điều đó, đúng không? Và, nên nó là một khu vực nghiên cứu rất hiếm, rất thú vị, đây là khoảng năm 2006, đúng không? Và, không có nhiều thứ đang xảy ra, nhưng tôi, tôi là, bạn biết đấy, tôi xem một bài giảng của, Geoffrey Hinton và cũng, và, và Geoffrey Hinton rất truyền cảm hứng, đang nói về cùng một trực giác rằng, bạn biết đấy, chúng ta, chúng ta phải học end-to-end, bạn biết đấy, chúng ta phải học tất cả các đặc trưng trong, trong cái kernel và những thứ như vậy, nên tôi rất quan tâm, nhưng, nhưng rồi cũng, xem một số bài nói chuyện của Andrew nữa, và tôi thực sự thích động lực của anh ấy, anh ấy thực sự muốn xây dựng, để xây dựng như một mô hình giống như não bộ, cho AI, nên giống như lấy cảm hứng từ não bộ, kiến trúc cho, cho AI, đúng không? Như là, và tôi nghĩ, ôi, đây là một điểm rất thú vị vì chúng ta chỉ có một, kiểu như một mảnh bằng chứng của trí thông minh, đó là não bộ, đúng không? Như là chúng ta chỉ có một mảnh bằng chứng và chúng ta có, cái đó thực sự trông giống như thông minh, đó là não của chúng ta, và nếu chúng ta muốn tái tạo trí thông minh, có lẽ nó sẽ đi qua, qua cùng con đường, như là, như tiến hóa đã tạo ra chúng ta, đó là một kiểu trực giác kỳ lạ thật sự, nhưng tôi, tôi nghĩ, bạn biết đấy, nó rất có lý, nên tôi đã, tôi bắt đầu làm điều đó, và, tôi có tất cả trực giác nhưng tôi nghĩ lời giải thích mà tôi có cho việc tại sao, tiến hóa và, trí thông minh sinh học và trí tuệ nhân tạo sẽ đi theo con đường rất, rất giống nhau là ý tưởng về tính song song, và hóa ra rằng thực ra nếu bạn muốn xây dựng một máy tính mà thực sự rất nhanh, bạn biết đấy, chỉ một GPU rất nhanh, nó thực sự rất khó, vào một thời điểm nào đó bạn thực sự muốn song song hóa, xử lý song song, và đó cơ bản là điều xảy ra trong não bộ, nên mỗi nơ-ron trong não bộ thực ra khá chậm, thực ra không nhanh hơn, băng thông kết nối và những thứ như vậy thực ra chậm, đúng không, đúng không? Nhưng có rất nhiều tính song song, và tôi nghĩ, ôi, đó, đó thực sự tuyệt vời rằng tính song song này, bạn biết đấy, mặc dù thực tế rằng mỗi, mỗi con chip rất chậm và kết nối rất chậm, nó vẫn tạo ra trí thông minh đáng kinh ngạc, và tôi nghĩ đó cơ bản là đưa tôi vào con đường này của, bạn biết đấy, như là chúng ta có thể khám phá, kiến trúc giống như não bộ cho trí thông minh không.
Christopher Nguyen: Tôi muốn thực sự nhấn mạnh với khán giả, bạn đã nghe điều này trước đây nhưng nó thực sự đúng và tôi có thể chứng thực điều đó, rằng, đó là một quyết định rất khó khăn, rất táo bạo, đó là một quyết định điên rồ vì đối với tôi trong kinh nghiệm cá nhân của tôi giữa năm 2000 và 2005, tôi đang làm việc về cái gọi là kinh doanh chênh lệch giá thống kê, đó là thứ ở Phố Wall, đúng không? Và, và đây là nơi mà mọi người kiếm rất nhiều, rất nhiều tiền bằng cách sử dụng các phương pháp thống kê, và vào thời điểm đó mọi người cũng đang cố gắng làm mạng nơ-ron, và tôi đảm bảo với bạn, những người trong chúng tôi trong lĩnh vực này nghĩ rằng những người làm mạng nơ-ron không chỉ ngây thơ mà như là điên rồ, đúng không? Đây chỉ là, điên rồ, hoặc tốt nhất là những người mơ mộng, những người không biết mình đang làm gì, cơ bản bị xem nhẹ đến mức độ đó, đúng không? À, nhưng bạn biết đấy, bạn theo đuổi niềm tin của bạn, đam mê của bạn, bạn làm việc với Andrew Ng, và rồi, cả thế giới biết đến điều này là bài báo năm 2012, từ The New York Times mà hầu hết mọi người gọi là “con mèo”, đúng không? Nhưng tôi muốn hỏi bạn về nó vì tôi nghĩ mọi người biết rằng bạn không thực sự tìm kiếm mèo, bạn đang tìm kiếm thứ gì khác, bạn có thể chia sẻ câu chuyện đó không?
Quoc V. Le: Được rồi, chắc chắn, nên có thể có một chút bối cảnh một chút, nên, tôi ở Stanford, từ, nên giữa năm 2007 và đến 2012, và trong thời gian đó, bạn biết đấy, chúng tôi, chúng tôi đang nghĩ về các thuật toán, đúng không? Như là chúng tôi đang cố gắng để, chúng tôi đang cố gắng cải thiện thuật toán, thuật toán học sâu, kiểu như vậy, và rồi, bạn biết đấy, chúng tôi, chúng tôi có một số kết quả hứa hẹn, bạn biết đấy, xây dựng và bộ mã hóa tự động, làm RBM thưa thớt và những thứ như vậy, rất nhiều công việc này, nhưng một điều mà chúng tôi phát hiện ra rằng thực ra mỗi khi chúng tôi công bố một bài báo, vì lý do nào đó chúng tôi có kết quả tốt hơn, đúng không? Như là và rồi chúng tôi nói tại sao, đúng không? Tôi đôi khi tôi lùi lại và tôi nói tại sao, và rồi tôi nhận ra rằng mỗi khi bạn có kết quả tốt hơn là vì người tiếp theo thực sự có một con chip tốt hơn, nên họ huấn luyện nhanh hơn hoặc, và họ huấn luyện lớn hơn, nên sự so sánh thực ra không phải là táo với táo, nó thực ra là, cái trò chơi cơ bản này đến từ máy tính và quy mô, nên tôi nói, được rồi, bây giờ, bây giờ tôi không thể, tôi không thể cứ tiếp tục điều chỉnh nữa, tôi cần mở rộng quy mô, và rồi, bạn biết đấy, như là, và rồi bạn biết đấy, chúng tôi, kiểu như được rồi, nếu chúng tôi muốn mở rộng quy mô, thì cái, cái gì, công ty nào ở khu vực Vịnh, bạn biết đấy, gần Stanford có nền tảng tính toán lớn nhất để bạn có thể mở rộng quy mô, và rồi tất nhiên đó là Google, đúng không? À, tham vọng của Google luôn là về quy mô, đúng không? Như là, nó, nó muốn làm mọi thứ ở quy mô lớn, nên chúng tôi tìm thấy, bạn biết đấy, và rồi bạn biết đấy, và nói chuyện với Larry Page và, bạn biết đấy, Sergey Brin và Jeff Dean, và rồi tôi đến Google và làm việc về cái mở rộng quy mô lớn này, mạng nơ-ron, nên hóa ra rằng đây là lần đầu tiên huấn luyện mạng nơ-ron quy mô lớn bao giờ trong ngành công nghiệp, thực ra ngày nay bạn thấy những khối mà nơi mọi người huấn luyện GPT lớn hơn, LLaMA lớn hơn và những thứ như vậy, thực ra nếu bạn nhìn ngược lại tất cả con đường, điều này bắt đầu vào khoảng năm 2012 nơi mà chúng tôi đặt cùng nhau dự án này để huấn luyện mạng nơ-ron lớn, được rồi, nên một sở thích mà tôi có, trong khi làm tiến sĩ của tôi là cái rất thú vị này được gọi là, nơ-ron bà ngoại, nên cơ bản trong não của bạn, mỗi, bạn có mọi người có một nơ-ron mà họ gọi là nơ-ron bà ngoại liên kết với bà của bạn, cơ bản bạn, bạn biết đấy, như là nếu bạn, nếu bạn thấy hình ảnh của bà bạn, cái nơ-ron đó được kích hoạt, và rồi nếu bạn nghe từ, tên của bà bạn, cùng cái nơ-ron đó được kích hoạt, hoặc nếu bạn thấy văn bản liên quan đến cái này, bà của bạn, cái nơ-ron này được kích hoạt, nên tôi tìm thấy, tôi tìm thấy điều đó thực sự hấp dẫn, cơ bản nó là nơ-ron đang xây dựng một dạng trừu tượng nào đó và tôi, tôi rất quan tâm đến sự trừu tượng vì sự trừu tượng, nếu bạn nghĩ về toán học, hoặc, sáng tạo hoặc đổi mới, đúng không? Đổi mới, chứng minh một định lý, cái, tạo ra một chiếc máy bay mới là một sự trừu tượng, nên tôi rất quan tâm đến ý tưởng về sự trừu tượng và làm thế nào để khiến mạng nơ-ron học sự trừu tượng, nên tôi thấy bài báo này và tôi rất quan tâm, đúng không? Và trong bài báo họ nói rằng, thực ra, ở Mỹ mọi người đều có một nơ-ron Jennifer Aniston, nên, nên nơ-ron Jennifer Aniston là, bởi vì hồi đó Friends là một chương trình TV rất nổi tiếng ở Mỹ, nên mọi người ở Mỹ có một nơ-ron cho Jennifer Aniston, bạn có thấy hình ảnh của Jennifer Aniston thì nó được kích hoạt, đây là thế hệ chia đôi khán giả, một số người nhận ra cái tên Jennifer Aniston và những người không, đúng không?
Christopher Nguyen: Đúng, đúng, đúng, đúng, ý tôi là, cái tương đương của ngày nay sẽ là nơ-ron Taylor Swift, và rồi tôi, tôi nghĩ, tôi thú vị, nên mục đích của tôi, trong dự án đó là, để phát hiện ra một nơ-ron Jennifer Aniston, tôi thực ra quan tâm, tôi, tôi, tôi yêu chương trình đó nữa, nên tôi như là, chúng ta có thể tìm một Jennifer, bạn nhìn vào cái này, vào thời điểm đó Jennifer có ảnh ở khắp mọi nơi, chúng tôi không tìm thấy, chúng tôi không tìm thấy một nơ-ron Jennifer Aniston, tìm thấy như một nơ-ron khuôn mặt người, như một khuôn mặt, bởi vì nếu bạn xem, nếu bạn cho nó YouTube, có rất nhiều, video nơi mà mọi người, bạn biết đấy, bạn biết đấy, làm như ngồi trước camera nói chuyện, đúng không? Và rồi chúng tôi cũng, có một nơ-ron cho mèo, đúng không? Và, tôi cũng là một fan lớn của mèo, nên, The New York Times khi mà, sau khi chúng tôi phát hiện ra những nơ-ron này, The New York Times, nhà báo thực ra, nói chuyện với chúng tôi và, bạn biết đấy, họ thấy khái niệm về, mèo siêu hài hước vì, bạn biết đấy, như là, họ nói, bạn biết đấy, như là nội bộ tại Google chúng tôi nói đùa rằng 90% video YouTube là mèo, đúng không? Nên cái, cái nhà báo thấy nó thực sự hài hước, nên họ, họ nói về mèo rất nhiều, nhưng, bạn biết đấy, đó là một số bối cảnh về câu chuyện, làm thế nào mà nơ-ron mèo ra đời.
Christopher Nguyen: Vậy nên tôi có một câu hỏi cá nhân hóa hơn nữa, rồi chúng ta sẽ chuyển sang các chủ đề kỹ thuật hơn, đúng không? Và đây là một câu hỏi nặng ký vì bạn đã có một lời mời để đi đến CMU với tư cách là một thành viên giảng viên, đúng không? Một trường đại học rất danh giá, đúng không? Nhưng hãy nói cho chúng tôi biết bạn đã quyết định ở lại Google và phát triển mọi thứ mà chúng ta thấy ngày nay như thế nào?
Quoc V. Le: À, đúng vậy, tôi, bạn biết đấy, tôi thực sự, đó thực sự là một quyết định lớn, tôi, tôi nghĩ tôi sẽ, tôi sẽ đi, đến CMU để trở thành một giáo sư, bạn biết đấy, như là, trong thời gian của tôi với tư cách là một sinh viên đại học và rồi nhà nghiên cứu và rồi một tiến sĩ, tôi luôn muốn trở thành một nhà nghiên cứu và, và nhà nghiên cứu của tôi vào thời điểm đó thực sự là con người để trở thành một giáo sư vì trong ngành công nghiệp không thực sự có một công việc nhà nghiên cứu trong những cái lớn này cho học máy thực sự, bởi vì, hầu hết các công việc lớn, giống như kỹ thuật hơn, đúng không? Tôi muốn trở thành một nhà nghiên cứu và giáo sư là như là tôi mặc định, cái công việc, nhưng, bạn biết đấy, tôi, bạn biết đấy, tôi nói chuyện với rất nhiều bạn bè và, bạn biết đấy, mọi người trong nhóm Brain, tôi thực sự thích nhóm Brain, nhóm Brain vào thời điểm đó, tôi thực sự, bạn biết đấy, chúng tôi có một môi trường thực sự tuyệt vời và tôi nói chuyện với bạn và tôi nói chuyện với một số bạn bè của bạn bao gồm bạn, tôi nói chuyện với bạn tại sao tôi, như là câu hỏi, đúng không? Và bao gồm bạn và tôi nghĩ bạn đã cho tôi cái nhìn rất thú vị này rằng, bạn biết đấy, cái này, cái này, cái này, cái mở rộng quy mô này, đúng không? Sẽ là cơ bản, không chỉ cho sự phát triển AI mà cơ bản, cho những công ty này, bởi vì những công ty này cuối cùng sẽ trở thành công ty AI và tôi nghĩ đó thực sự là, đó là một cái nhìn thú vị, tôi, tôi nghĩ, bạn biết đấy, Google muốn trở thành như một công ty tìm kiếm hoặc Microsoft muốn trở thành như một công ty đám mây hoặc chúng tôi, công ty hệ điều hành nhưng hóa ra rằng thực ra các công ty lớn muốn trở thành công ty AI và đó là một điểm rất thú vị, nên tôi nghĩ, ôi, nên cái này sẽ là tương lai của ngành công nghiệp, đúng không? Và, nó sẽ là tương lai của Google, sẽ là tương lai của ngành công nghiệp nên tôi sẽ tìm chỗ của tôi nên bạn biết đấy, cái nhìn của bạn và, có một, bạn biết đấy, quá sớm để rút ra một bài học nhưng tốt để có, có một bài học ở đây nơi mà tài năng có nghĩa là tài nguyên, đúng không? Nó giống như ngã ba của nơi mà Việt Nam đang ở ngày nay, đúng không? À, hãy, hãy chuyển sang những thứ kỹ thuật hơn bằng cách nói về công việc mà bạn nhận được gần đây, giải thưởng Test of Time, cuối cùng chúng tôi, chúng tôi luôn biết nó là một phần của cái đó, nó mất, bạn biết đấy, giải thưởng Test of Time mất 10 năm trước khi họ có thể, họ có thể trao giải đó, nhưng hãy kể cho chúng tôi về mối liên kết từ sequence to sequence mà tất nhiên có những tiền thân từ đó đến Transformer thiết yếu và rồi phần còn lại của những gì chúng ta thấy bạn và rồi, rồi tôi muốn nói chuyện với bạn về tương lai, hãy bắt đầu từ, đúng không?
Quoc V. Le: À, có một chút bối cảnh trước, và điều này cho thấy rằng, trong nghiên cứu và trong ngành, có rất nhiều yếu tố của may mắn và rất nhiều, kiểu như không có kế hoạch, bạn biết đấy, như là không có kế hoạch, kiểu như khó để lập kế hoạch cho những khoảnh khắc như thế này, đúng không? À, nên trước đó tôi muốn nói rằng cái mà Christopher đề cập đến là bài báo sequence to sequence của tôi giành giải thưởng Test of Time tốt nhất tại NeurIPS, đây là một trong những giải thưởng lớn trong lĩnh vực cho, cho bài báo mà, bạn biết đấy, thể hiện tác động nhất quán trong một thập kỷ, lý do là như là, tôi, chúng tôi bắt đầu công việc này khoảng năm 2013, và lịch sử là rằng tôi đã làm, một số mạng nơ-ron đầu tiên của tôi cho cái này, nơ-ron mèo và những thứ như vậy tại Google, họ gọi nó là QuocNet, và cái QuocNet trở nên khá là hữu ích tại Google nên rất nhiều người dùng nó, bạn biết đấy, cho rất nhiều nhiệm vụ thị giác máy tính nhưng rồi khu vực đó trở nên rất đông đúc nhưng mọi người như là tôi phải đi họp và rất nhiều người và những thứ như vậy, đúng không? Nên và như là nhà nghiên cứu tôi cảm thấy như là tôi muốn, tôi, tôi muốn thử cái gì đó thú vị, đúng không? Tôi không, và rồi tôi đang khám phá, cố gắng để có được, cố gắng để tốt hơn mạng nơ-ron và những thứ như vậy, nhưng rồi từ từ tôi, tôi, tôi, tôi bắt đầu nhận ra rằng, thực ra nếu bạn muốn xây dựng trí thông minh, văn bản sẽ là một yếu tố rất then chốt, văn bản sẽ là then chốt, bởi vì nếu bạn muốn có như là một AI, bạn muốn đến AI và bạn muốn hỏi nó đang nghĩ gì, đúng không? Và bạn có thể nói, nếu bạn muốn nó phát minh ra như là một loại thuốc, đầu ra nên trông như là một văn bản, đúng không? Hoặc nếu bạn muốn, để phát minh ra như là một vắc-xin, đầu ra nên là văn bản, là văn bản như là một tương tác phổ biến hơn với con người và AI, đúng không? Nên và nó cũng quay lại những ngày đầu của tôi ở trung học, tôi là, bạn biết đấy, như là cố gắng xây dựng chatbot bằng công cụ thủ công và những thứ như vậy nhưng không thành công, nên, rồi bạn biết đấy, tôi bắt đầu làm việc về, bạn biết đấy, như là một, một, của tôi, tham vọng của tôi là làm việc về cái gì đó, cái gì đó cơ bản là một mô hình end-to-end cho tất cả NLP, như tất cả xử lý ngôn ngữ tự nhiên nên được làm bởi một mô hình, đó là tầm nhìn của chúng tôi, được rồi, bạn biết đấy, cái đó khá là hài hước vì tôi chưa bao giờ học bất kỳ lớp nào về NLP thực sự, nên tôi, tôi nghĩ, bạn biết đấy, có lẽ mạng nơ-ron có thể làm học end-to-end, có lẽ chúng ta nên đi học end-to-end cho tất cả NLP, đó thực ra là một lợi thế vì, bạn biết đấy, nhiều bạn bè của bạn, những người cùng thời quen thuộc với NLP đã bị kẹt trong cái tương đương với phương pháp kernel, nếu bạn sửa lại, đây là lý do tôi may mắn trong ý nghĩa rằng vì sự ngây thơ của tôi đã giúp tôi một chút, bởi vì hầu hết bạn bè của tôi quen thuộc với NLP sẽ phản đối kiểu ý tưởng này, đúng không? Nhưng tôi, chúng tôi cuối cùng đã thử dù sao, ý tôi là nó mất chúng tôi như 9 tháng, để làm dự án này và tôi có cơ hội làm việc với một số nhà nghiên cứu tốt nhất trong lĩnh vực của chúng tôi vào lúc đó, đúng không? Ilya và Oriol là như tuyệt vời, nhà nghiên cứu, nên tôi có cơ hội học từ họ nữa, nhưng để kết nối với điểm mà bạn hỏi về, Transformer, cái mạng này, thực ra, chúng tôi, tầm nhìn của chúng tôi là có một mạng làm hiểu ngôn ngữ end-to-end nhưng cho ứng dụng đầu tiên, cái rõ ràng nhất bạn phải thuyết phục mọi người rằng đây là một khung khả thi, đúng không? Bạn không thể chỉ nói đây là khung, bạn thực sự cần bằng chứng thực nghiệm rằng cái này sẽ hoạt động, nên bằng chứng thực nghiệm đầu tiên mà chúng tôi chỉ ra là, dịch máy vì dịch máy, trong dịch máy bạn có rất nhiều dữ liệu, được rồi? Nên chúng tôi, chúng tôi thực ra, trong bài báo của chúng tôi, nếu bạn đọc bài báo, tầm nhìn thực ra là end-to-end cho mọi thứ nhưng kết quả trên bài báo chỉ là về dịch máy, bây giờ Transformer thực ra là cái, sau khi chúng tôi công bố bài báo, đồng nghiệp của chúng tôi thực ra cố gắng nghĩ quanh ý tưởng này về làm thế nào để làm, cái này, hồi đó chúng tôi dùng cái gọi là LSTM, cái là một mạng bộ nhớ ngắn dài, đúng không? À, và rồi sau khi chúng tôi công bố bài báo, cũng có một bài báo ở Montreal dùng LSTM với, chú ý và tôi, tôi nghĩ, bạn biết đấy, một khi bạn có chú ý và rồi LSTM, bạn bắt đầu thấy xử lý song song hơn, một điều mà chúng tôi nhận ra trong quá trình phát triển LSTM là nó tuần tự, đúng không? Và nó không thân thiện với GPU và TPU, chúng tôi muốn cái gì đó song song hơn, và rồi khi chúng tôi thấy, Transformer, chú ý, chúng tôi nhận ra rằng chúng tôi bắt đầu có cái gì đó trông giống như xử lý song song hơn, rồi đồng nghiệp của chúng tôi thực ra lấy khung sequence to sequence nhưng bỏ đi tất cả kết nối ngang và chỉ giữ, kết nối tiến và chú ý và cái đó trở thành, bạn biết đấy, Transformer thực sự, và nên bạn sẽ nói rằng, bạn biết đấy, như một số phát triển này là sequence to sequence, xây dựng nền tảng, để cơ bản đổi thời gian lấy không gian, bạn có thêm không gian, bạn có thể làm mọi thứ lại song song, đúng không?
Christopher Nguyen: À, có thể một câu chuyện bên lề nữa, có lẽ mọi người nhận ra bạn đã đề cập đến Oriol và bạn đã đề cập đến Ilya này và, Ilya thực ra như một số bạn biết đã đi để đồng sáng lập, Open AI và đang cố gắng kéo bạn vào đó nữa và trong một thời gian bạn đang cân nhắc điều đó nhưng tôi, tôi rất vui vì bạn đã quyết định ở lại, ở lại Google, đúng không? Tất nhiên mọi thứ đã diễn ra và, và bạn đã có rất nhiều tác động ngày nay, hãy đi qua thật nhanh qua cơ bản cái gì bây giờ là Gemini, đúng không? Bây giờ con đường đến đó không bao giờ thẳng, đúng không? Có những lựa chọn khác như PaLM và, và những kiến trúc khác, cho đến khi sáp nhập của bạn trong Mind, quá trình quyết định để đến Gemini là gì, đúng không? À, câu hỏi được hỏi là nó khó hay dễ, nó rõ ràng hay là nó, nó không rõ ràng, câu hỏi nóng, nên nhưng tôi có thể kể cho bạn một chút lịch sử một chút trước, đúng không? Nên nội bộ, khoảng năm 2017 có hai dự án, một là BERT mà thực ra rất nổi tiếng, nên đây là như kiểu mã hóa chỉ, kiểu kiến trúc và nó kiểu như nhằm xây dựng một nhúng rất tốt cho, bạn biết đấy, hoặc một câu hoặc một tài liệu và nó vẫn rất được dùng tốt cho nhiều mục đích, chúng tôi, bạn để đọc về phía tạo sinh, mọi người không dùng nó bình thường cho, cho, cho giải mã nhưng, đúng không? Một số, đúng không? Ý tôi là nơi bạn không cần phía đó, nó vẫn rất hữu ích ở phía mã hóa, đúng không?
Quoc V. Le: Chắc chắn, để tôi, để tôi hoàn thành câu chuyện của tôi trước, câu chuyện đằng sau cái này trước, nên, nên cái này BERT và có một dự án ít nổi tiếng hơn nhiều tại đây gọi là Lambda và dự án Lambda là để xây dựng một chatbot end-to-end, và cơ bản tôi, tiền thân của cái gì đó như một chatbot bây giờ, nội bộ hầu hết mọi người, phần lớn mọi người thực ra đi theo BERT vì BERT, với họ là rất hữu ích, bạn biết đấy, bạn có thể tích hợp với tìm kiếm, bạn có thể tích hợp với quảng cáo, bạn biết đấy, rất nhiều những, sản phẩm này cho Google, cho Lambda thực ra phần lớn mọi người phản đối và lý do là có quá nhiều ảo giác, nó thực ra có thể nói ra rất nhiều như là những thứ không an toàn, đúng không? Tôi nói, bạn biết đấy, như là đôi khi nó nói rất nhiều điều xấu bây giờ, nên nhưng niềm tin của tôi thực ra là cái này, mô hình chỉ giải mã là mạnh mẽ hơn mô hình mã hóa và lý do là mô hình giải mã tổng quát hơn mô hình mã hóa vì bạn có thể dùng mô hình giải mã cho mã hóa nữa và bạn có thể dùng nó để tạo sinh khi rất mạnh mẽ và tạo sinh sẽ là, một sự thay đổi mô hình cho Google, Google chưa bao giờ muốn tạo sinh, chỉ muốn là, bạn biết đấy, phục vụ nội dung hiện có, đúng không? Như là, bạn biết đấy, bạn không tạo sinh nội dung mới thực ra rất khó cho bạn, nhưng niềm tin của tôi là rằng, bạn biết đấy, nếu bạn nghĩ trước đường cong như 10 năm, vào một thời điểm nào đó bạn sẽ tìm ra cách để giảm ảo giác, giảm vấn đề an toàn và rồi thay vì đọc qua 10 liên kết để đọc nội dung, bạn thực sự có nội dung trước mặt bạn, bạn biết đấy, như một đoạn văn cho một câu trả lời, nên tôi nghĩ, bạn biết đấy, trực giác nó có lý, như là mô hình tạo sinh siêu mạnh mẽ, nên ai đó nên làm việc với nó, nên chúng tôi làm việc với nó, khoảng 10 người, đúng không? Và trong nên nó rất nhỏ nhưng vì thiếu đầu tư, rất khó để bao giờ ra mắt một cái công khai, nhưng chúng tôi, chúng tôi thực ra phát hiện ra một loạt thứ mà bạn thấy trong ChatGPT, bạn biết đấy, như tinh chỉnh hướng dẫn, an toàn, giảm ảo giác, tất cả những thứ này thực ra chúng tôi phát triển trong cái thực sự bạn tìm thấy trong bài báo Lambda, không may chúng tôi, không có cơ hội để, ra mắt nó, trước, tôi nghĩ nếu chúng tôi ra mắt nó khoảng năm 2020, đó 2019 hoặc 20, đó sẽ là một cú hit lớn, đúng không? Điều đó sẽ thay đổi dòng chảy của lịch sử rồi, nhưng, bạn biết đấy, chúng tôi quyết định không và, đúng vậy, tôi nghĩ đó, tôi sẽ nói gì, đó là một quyết định khó, đúng không? Tôi nghĩ nó rất khó, đúng vậy, rất khó cho mọi người, tôi cũng vậy, tôi nghĩ tôi, tôi luôn cảm thấy với tư cách là một nhà nghiên cứu, tôi luôn cảm thấy rằng, bạn biết đấy, ảo giác và an toàn sẽ mất rất lâu để, để giải quyết và mất nhiều thời gian hơn, và ChatGPT đến như một bất ngờ, đúng vậy, chắc chắn, lại nhìn lại thì rất rõ ràng, điều ấn tượng với mọi người rằng con đường nghiên cứu và phát triển rất là, như là dịch thuật thì giá trị rõ ràng hơn nhiều, đúng không? Bạn, tiếng Pháp sang tiếng Anh, tiếng Pháp, được rồi, hãy làm điều đó, nhưng chỉ ngồi đó và tạo sinh văn bản, đặc biệt là nói chuyện khi đang tạo sinh, đúng không? Và rồi nói, tôi nhớ bạn đang chỉ cho tôi kết quả trên bảng trắng với cái Stanford với câu hỏi Stanford SQuAD, đó chỉ là, đó chỉ là ngẫu nhiên, đúng không? Nó không làm gì cả, nhưng với quan điểm đó trong đầu bây giờ, hãy, bây giờ nhìn về tương lai, đúng không? À, quan điểm của bạn và quan điểm của Google về kiến trúc Transformer, tuổi thọ của chúng là gì, đúng không? Và rồi tôi muốn hỏi về suy luận nhưng chỉ, chỉ là kiến trúc chung của trí thông minh, bạn có nghĩ đó là tối thượng, chúng ta chỉ cần mở rộng nó, đúng không? Như một số người nghĩ, hay chúng ta cần một, bạn biết đấy, đây chỉ là một, một, một số người thậm chí sẽ nói là đường vòng đến trí thông minh, chúng ta thực sự ra khỏi trí thông minh, đúng không?
Quoc V. Le: À, câu hỏi của bạn là, quá tải và có nhiều thứ trong đó, nhưng bạn có thể đi theo bất kỳ hướng nào, được rồi, nên để tôi nói, trước tiên về mặt quy mô, ý tôi là bạn, chúng ta, chúng ta chắc chắn không nên đánh giá thấp sức mạnh của việc mở rộng quy mô, tôi nghĩ đây là, bạn biết đấy, lần nào cũng vậy, đúng không? Chúng ta thấy rằng chỉ như là, bạn biết đấy, thuật toán cơ bản mà bạn, bạn có với dữ liệu nhiều hơn, tính toán nhiều hơn, mô hình lớn hơn, bạn có thể đi rất xa, đúng không? Thậm chí, bạn biết đấy, một số cái ảo giác này thực ra có thể được giải quyết bằng quy mô lớn hơn, đúng không? Nên tôi sẽ không đánh giá thấp điều đó, nhưng tôi nghĩ chúng ta có thể, nên chúng tôi đã đưa ra một tại cái mới, tại cái trình bày giải thưởng Test of Time mới, chúng tôi nói về cái này, và chúng tôi nói rằng huấn luyện trước đặc biệt sẽ, sẽ đến, đến huấn luyện trước, đây là một khối xây dựng rất cơ bản của một, chúng tôi nói trong bài thuyết trình đó và đây là rất khiêu khích, đúng không? Rất nhiều nhà báo viết về nó, nhưng chúng tôi đi đến một thỏa thuận rằng huấn luyện trước sẽ đạt đến giới hạn, thực ra có thể trong 6 tháng nữa, có thể mọi người sẽ tiếp tục làm nó trong một thời gian nhưng tôi nghĩ nó không phải, nó sẽ chạm đến hiệu suất giảm dần, nên đó là số một, tôi nghĩ mọi người sẽ cố gắng tìm ra cách để mở rộng tất cả các khía cạnh, nên nếu bạn nghĩ về mở rộng Transformer, bạn biết đấy, bạn mở rộng số lượng tham số, số lượng chuyên gia, số lượng lớp, nhưng ngay bây giờ với mô hình suy luận, họ đang cố gắng mở rộng số lượng, bước của các bước suy nghĩ của Transformer, những suy nghĩ, đúng không? À, nhân tiện chúng tôi cũng nhận ra điều này, đúng không? Sớm hơn vì cái chuỗi công việc, nên tôi, tôi là đồng tác giả của một số công việc chuỗi suy nghĩ nữa và chúng tôi nhận ra rằng thêm tính toán vào Transformer sẽ vào thời điểm đó cho bạn khả năng giải quyết tốt hơn, nên chúng tôi tìm thấy nó, trong 2020, cuối 2021, dù sao, bây giờ câu hỏi của bạn, câu hỏi khó hơn của bạn là liệu Transformer sẽ ở lại hay sẽ tồn tại, tôi sẽ nói thế này, đó là một câu hỏi rất thú vị nhưng tôi nói tôi thực sự thích Transformer vì điều chính, điều chính mà bạn muốn, bạn muốn học từ Transformer là ý tưởng rằng tính song song rất quan trọng, bạn không muốn xử lý tuần tự, bạn muốn tính song song vì xây dựng nhiều chip hơn là tốt hơn là xây dựng một, như là một con chip siêu đắt tiền, đúng không? Nên vì lý do đó tính song song là, là rất mạnh mẽ, nên Transformer chỉ là một cơ chế để khai thác tính song song thực sự, bây giờ, tôi nghĩ cái chuỗi suy nghĩ dài này, chuỗi suy nghĩ dài, cái suy luận này thực ra có thể là bước đầu tiên mà bạn có thể thấy nơi mà Transformer, có thể không tốt lắm, và tôi nghĩ, tôi nghĩ đây có thể là lần đầu tiên mà tôi cảm thấy như nó có thể bị thách thức, đúng vậy, nhưng tôi nghĩ mọi người sẽ tìm ra cách để làm nó hiệu quả hơn và những thứ như vậy, nên câu hỏi là chúng ta có đang ở một đường vòng không và không, tôi không nghĩ vậy, tôi nghĩ, tôi nghĩ, tôi nghĩ chúng ta đang đi đúng hướng nào đó tốt, bây giờ vấn đề là chúng ta sẽ, chậm lại một chút trong một thời gian và lý do là vì, bạn biết đấy, hầu hết các nhiệm vụ mà chúng ta cố gắng làm như dự đoán mã thông báo tiếp theo với huấn luyện trước và cái này, cái này và những thứ như vậy có rất nhiều như là bạn có thể tìm rất nhiều dữ liệu cho cái này, nhưng để đến cấp độ tiếp theo, lấy dữ liệu cho nó là khó, ví dụ nếu bạn muốn khiến AI tiến bộ như vắc-xin tiếp theo cho ung thư, hãy làm một thí nghiệm hàng đầu, đúng không? Nếu bạn muốn làm điều đó bây giờ thì cái, cái phần thưởng, phần thưởng để bạn huấn luyện mô hình đó thực ra rất đắt, bao gồm làm thí nghiệm thực tế và cái đó có thể mất lâu hơn nhiều để tích lũy dữ liệu, tôi, đây là như một thí nghiệm tư duy cực đoan, đúng không? Nhưng bạn có thể làm một thí nghiệm tư duy nhỏ, nếu bạn có thể nói, bạn có thể khiến AI đó làm một số, bạn biết đấy, bài kiểm tra trợ lý cho bạn, đúng không? Như là, lên mạng mua cho tôi, mua cho tôi một cuốn sách, mua cho tôi một cuốn sách mà bạn biết đấy, con gái của bạn tôi sẽ thích vì bạn biết đấy, cô ấy quan tâm đến vật lý, cái gì đó như vậy, nó sẽ phải đi để hiểu tất cả, bạn biết đấy, email của bạn, tất cả những thứ này, đúng không? Điều đó có lẽ yêu cầu, rất nhiều chuyên môn để huấn luyện mô hình và, và số lượng dữ liệu mà bạn có được cho nó thì hạn chế hơn nhiều, bất kỳ thứ gì chạm đến thế giới vật lý sẽ chậm hơn rất nhiều, rất nhiều, nên tôi không nghĩ chúng ta đang ở đường vòng nhưng chúng ta sẽ bị trì hoãn và rồi, có lẽ, một thứ khác mà chúng ta sẽ thấy là rằng tôi nghĩ, mọi người, nếu bạn nghĩ về các mã thông báo trên internet, một kiểu mã thông báo trên internet thực sự có giá trị là video nhưng, nhưng cái, cái thông tin trong video thực ra siêu thấp, nên bạn, bạn cần nhiều tính toán hơn để đến bất kỳ thứ gì mà tôi thực sự trở nên hữu ích, nên vì những thách thức này bạn sẽ bị trì hoãn và chúng có thể, nó có thể kéo dài trong, bạn biết đấy, vài năm, tôi không biết, có lẽ một thập kỷ, tôi không biết, tôi nghĩ mọi thứ đến nhanh hơn tôi, dự đoán nhưng tôi nghĩ nó sẽ như là một sự trì hoãn, nhưng tôi không nghĩ đó là một đường vòng, tôi nghĩ, tôi nghĩ dự đoán mã thông báo tiếp theo là một mô hình rất quan trọng, tôi nghĩ cuối cùng chúng ta cũng là dự đoán mã thông báo tiếp theo, chúng ta chắc chắn làm rất nhiều khám phá, chúng ta chắc chắn có niềm tin, khám phá, trực giác mà những mô hình này chưa có và tôi nghĩ cái đó có thể là cái gì đó mà, bạn biết đấy, như là của tôi có thể tạo ra khung mới để huấn luyện những mô hình này nhưng tôi nghĩ cái đó giống như là, cái mục tiêu và khung học tăng cường để huấn luyện nó nhưng tôi nghĩ các khối xây dựng cơ bản khá là hài lòng để, để tốt.
Christopher Nguyen: Nên tôi sẽ chia sẻ với bạn niềm tin của tôi liên quan đến cái đó và rồi xây dựng trên đó hỏi tôi câu hỏi cuối cùng, và tôi đã chia sẻ điều này với bạn trước đây, tôi có cái này, nhưng nó đến từ một niềm tin rất mạnh mẽ rằng suy luận thực sự yêu cầu sự lặp lại, đó là có một vòng thời gian cần diễn ra, nó không phải là nối tiếp, nó không phải là, nó có thể được song song hóa nhưng rồi thời gian là một thành phần quan trọng và đây là kể từ khi bạn phát hiện, nếu bạn muốn, của suy luận chuỗi suy nghĩ dường như đang làm cái gì đó thú vị rằng, rằng chúng ta suy luận qua lời nói của chúng ta khi tôi, khi tôi đang nói ra lời, tôi đang nghe chúng và tôi đang suy luận qua chúng và, và thậm chí có bằng chứng rằng não bộ chúng ta có các kết nối lặp lại quay lại ngay cả trong lớp muộn trước khi chúng thậm chí trở thành lời, nên với tôi đó là một kiểu kiến trúc mới hoặc không mới nhưng bạn biết đấy, mới so với Transformer, đó là những biên giới thực sự thú vị cho 5 đến 10 năm tới như bạn nói, nói nếu bạn đang làm nghiên cứu, đừng làm việc với LLM, đúng không? Đó là kỹ thuật, nên câu hỏi của tôi với bạn và bạn có thể trả lời nó theo một hoặc hai hướng, bạn nghĩ gì về các hướng như trí nhớ JEA, mosaic và những thứ như vậy, bạn chung của chúng ta G đang ủng hộ cho các kiến trúc tương lai và một, một kiến trúc nơi mà có một cái mà tôi sẽ gọi là chức năng điều hành riêng biệt, cái khác với chức năng huấn luyện nơi mà nó có một mô hình thế giới, mục tiêu và những thứ như vậy, đó là một kiến trúc khác của trí thông minh hơn cái đang được theo đuổi hiện tại, cái là hãy cứ nhồi nhét mọi thứ vào những LLM này và suy luận, lập kế hoạch sẽ nổi lên, đúng không? Nên cái đó bạn có thể chia sẻ quan điểm của bạn hoặc bạn có thể chia sẻ Google đang làm gì, ôi?
Quoc V. Le: Được rồi, được rồi, ý tôi là, chúng ta đều nhất quán, đúng không? À, tôi nghĩ câu hỏi của bạn lại, câu hỏi của bạn có nhiều thứ trong đó nên tôi muốn chia nhỏ nó trước, trước tiên bạn chỉ trích, bạn, bạn muốn thấy nhiều sự lặp lại hơn, trong Transformer, đúng không? Chúng ta có một điểm tốt, tôi nghĩ cái này có thể, bạn đã bắt đầu thấy cái này, đúng không? Vì cái chuỗi suy nghĩ dài này, bạn có thể, có thể thấy cái đó nhiều hơn, nên, đúng vậy, tôi nghĩ đó là một điểm tốt, và rồi bạn chạm đến, cái này, mục tiêu cấp cao, mà, bạn biết đấy, Yann đang làm việc, thực ra, bạn biết đấy, tôi nghĩ, tôi nghĩ đó là, đó liên quan đến điểm của tôi về làm thế nào để huấn luyện mô hình này, của tôi thay đổi thời gian, bạn biết đấy, như là bởi vì tôi, tôi nói về ý tưởng này về việc có niềm tin và trực giác và niềm tin trong một số ý nghĩa là như một thứ cấp cao nơi mà bạn, bạn tin vào nó và rồi, bạn biết đấy, nó không như là một chuỗi suy nghĩ dài nhưng nó như là một tầm nhìn và rồi bạn theo đuổi tầm nhìn đó, đúng không? Và cái đó dẫn dắt bạn trong một cái rất, rất, thế giới thưa thớt, nên ví dụ niềm tin của tôi, đúng không? Như là nếu bạn nghĩ về niềm tin của tôi về mạng nơ-ron, đúng không? Thực ra hầu hết thời gian tôi không nhận được bất kỳ phần thưởng nào cả, như là trong 10 năm đầu, 10 năm làm việc với nó, không bao giờ nhận được bất kỳ phần thưởng nào, tôi chỉ nhận được sự hài lòng từ niềm tin này, nên đó như là một dạng phần thưởng mà cái này, cái mô hình, nên tôi không phản đối cái đó, tôi nghĩ rằng anh ấy, anh ấy đang đi đúng hướng, tôi, tôi không, tôi nghĩ rằng đó là, anh ấy có một điểm tốt, đúng vậy, tôi, tôi nghĩ, quan điểm của Google và, và quan điểm của tôi, tôi nghĩ chung nhất quán là rằng như là chúng ta đang đi đúng hướng nào đó tốt, chúng ta phải có thể phải thay đổi một vài thứ trong những mô hình này nhưng tôi nghĩ nó có thể trở thành như là, kiểu như lớp trên lớp của nhau hơn là, bạn biết đấy, như là dọn sạch nó và rồi cái gì đó ở đó, tuyệt vời, đúng không?
Christopher Nguyen: À, bạn có thể thấy tôi đã cố gắng khiến anh ấy nói cái gì đó về những gì Google đang làm việc công khai nhưng anh ấy sẽ không làm điều đó, nên để tiếp tục, được rồi, bạn cần đi ăn trưa, cảm ơn bạn rất nhiều, Quoc, và cảm ơn bạn vì tất cả những đóng góp của bạn cho thế giới và cho Việt Nam.
